{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04a9ae27",
   "metadata": {},
   "source": [
    "# Activity: Understanding the ROC and PR Curves\n",
    "\n",
    "In this activity, we'll build on material in the *Performance Measures* lecture to build our own receiver operating characteristic (ROC) curve and precision-recall (PR) curve.\n",
    "\n",
    "We'll begin by repeating steps we took in our last computational exercise. For details on this step, please refer to the exercise, which may be found [here](https://github.com/mengelhard/bsrt_ml4h/blob/master/notebooks/ce3.ipynb). In the exercise we quantified performance using accuracy and the area under the ROC curve (AUC). Here, we'll plot the ROC and PR curves on the test set for the predictions made by our logistic regression model.\n",
    "\n",
    "To get started, run the next 6 code blocks. If you're working in Colab, make sure to uncomment the `!pip install` line in the second block before you run it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a40f718",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-darkgrid') # IMPROVES FIGURE AESTHETICS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa396692",
   "metadata": {},
   "outputs": [],
   "source": [
    "### UNCOMMENT AND RUN THIS LINE IF WORKING IN GOOGLE COLAB ###\n",
    "#!pip install --upgrade scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92a53564",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "df, y_true = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "y_true = 1 - y_true # let's set benign to 0 and malignant to 1, in keeping with usual conventions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dc1de26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_some_labels(labels, flip_rate=.1, random_seed=0):\n",
    "    return (labels + (np.random.RandomState(random_seed).rand(len(labels)) < flip_rate)) % 2\n",
    "\n",
    "y = flip_some_labels(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ed87fb62",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = df[:400]\n",
    "X_test = df[400:]\n",
    "\n",
    "train_mean = X_train.mean()\n",
    "train_std = X_train.std()\n",
    "\n",
    "X_train = (X_train - train_mean) / train_std\n",
    "X_test = (X_test - train_mean) / train_std\n",
    "\n",
    "y_train = y[:400]\n",
    "y_test = y[400:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "762fda74",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression(random_state=0, max_iter=10000)\n",
    "lr_model.fit(X_train, y_train)\n",
    "\n",
    "y_test_pred_proba = lr_model.predict_proba(X_test)[:, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79b87151",
   "metadata": {},
   "source": [
    "## Building the confusion matrix\n",
    "\n",
    "In this section, we'll fill in the values of the 2x2 confusion matrix corresponding to a specific threshold of your choice.\n",
    "- In the first block, you'll choose a threshold and make binary (i.e. $\\{0, 1\\}$-valued) predictions about `y_test` by applying that threshold to `y_test_pred_proba`.\n",
    "- The second block will calculate the number of true positives for the threshold you've chosen. It is important to think carefully about what is happening in this calculation. In short, we are counting all the cases where both `y_test` and `y_test_pred_label` are 1.\n",
    "- In the third block, you should calculate the number of false positives, true negatives, and false negatives. Each can be calculated by modifying the line used to calculate the number of true positives.\n",
    "\n",
    "We now have our confusion matrix! Once you've completed these steps, try a few different values of `DECISION_THRESHOLD` to see how it affects the different values.\n",
    "\n",
    "Note: an alternative, useful method to calculate these values is to cross-tabulate `y_test` and `y_test_pred_label` with `pd.crosstab()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d055825e",
   "metadata": {},
   "outputs": [],
   "source": [
    "DECISION_THRESHOLD = 0.5\n",
    "\n",
    "y_test_pred_label = (y_test_pred_proba > DECISION_THRESHOLD).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ddf378c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 32 true positives\n"
     ]
    }
   ],
   "source": [
    "tp = np.sum((y_test == 1) & (y_test_pred_label == 1))\n",
    "\n",
    "print('There are', tp, 'true positives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "775fb6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count false positives\n",
    "# fp = \n",
    "\n",
    "# count true negatives\n",
    "# tn = \n",
    "\n",
    "# count false negatives\n",
    "# fn = \n",
    "\n",
    "# print all of the counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dff28640",
   "metadata": {},
   "source": [
    "## Calculate TPR, FPR, and PPV\n",
    "- The first block below calculates the true positive rate (i.e. sensitivity) based on the counts of true positives and false negatives from the previous block.\n",
    "- In the second, you should calculate the false positive rate (i.e. 1 - specificity) and positive predictive value (i.e. precision) in a similar manner based on counts from your confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9eb73bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpr = tp / (tp + fn)\n",
    "print('The true positive rate is', tpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4adaff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the false positive rate\n",
    "# fpr = \n",
    "\n",
    "# calculate the positive predictive value\n",
    "# ppv = \n",
    "\n",
    "# print both values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57d98a39",
   "metadata": {},
   "source": [
    "## Calculate the TPR, FPR, and PPV across a range of thresholds from 0 to 1\n",
    "\n",
    "Now we get to the fun part. We'd like to calculate the TPR, FPR, and PPV as before, but while moving the threshold between 0 and 1 in very small increments. We can do this with a simple `for` loop, as illustrated in the next block:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d640f393",
   "metadata": {},
   "outputs": [],
   "source": [
    "for item in ['keys', 'phone', 'mask']:\n",
    "    print('Don\\'t leave home without your', item, '!')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53f94f41",
   "metadata": {},
   "source": [
    "First, let's define the thresholds we'd like to use. Let's use `np.linspace` to create an array of 1000 evenly spaced values between 0 and 1. To visualize these values, it may help to print them. We'll then be able to use a `for` loop to see what happens when we use each of these values as a threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9057e44d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "thresholds = np.linspace(0, 1, 1000)\n",
    "print(thresholds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2fad5bc",
   "metadata": {},
   "source": [
    "Before starting our `for` loop, let's create empty lists that we'll use to store the TPR, FPR, and PPV values we'll be calculating for each threshold as we move through the loop. We'll then loop over the thresholds; note that each of the indented lines is executed within each loop iteration.\n",
    "\n",
    "We'll do a few things in our loop:\n",
    "1. Predict the labels by applying the current threshold value to `y_test_pred_proba`. This line does not need to be modified.\n",
    "2. Count true positives, false positives, true negatives, and false negatives based on the true labels and predicted labels. You'll need to add your code from previous blocks to do this.\n",
    "3. Based on these counts, calculate the current TPR, FPR, and PPV. Again, you'll need to add your code from previous blocks to do this.\n",
    "4. Append the TPR, FPR, and PPV values you just calculated to our growing list of TPRs, FPRs, and PPVs (for all thresholds). These lines do not need to be modified.\n",
    "\n",
    "**Important Note:** Python will raise a warning, because the PPV will be NaN for at least one of your thresholds (*Challenge*: which one??). Don't worry about this warning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "007936aa",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Create empty list for the true positive rates we'll be calculating in our loop\n",
    "tprs = []\n",
    "\n",
    "# Create empty list for the false positive rates we'll be calculating in our loop\n",
    "fprs = []\n",
    "\n",
    "# Create empty list for the positive predictive values we'll be calculating in our loop\n",
    "ppvs = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    \n",
    "    y_test_pred_label = (y_test_pred_proba > threshold).astype(int)\n",
    "    \n",
    "    # add your code to calculate true positives, false positives, true negatives, and false negatives\n",
    "    tp = np.sum((y_test == 1) & (y_test_pred_label == 1))\n",
    "    #fp = \n",
    "    #tn = \n",
    "    #fn = \n",
    "    \n",
    "    tpr = tp / (tp + fn)\n",
    "    #fpr = \n",
    "    #ppv = \n",
    "    \n",
    "    tprs.append(tpr)\n",
    "    fprs.append(fpr)\n",
    "    ppvs.append(ppv)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c29cc687",
   "metadata": {},
   "source": [
    "## Plot the ROC Curve!\n",
    "\n",
    "The code below is ready to go! `plt.plot()` is doing all the work here; the remaining lines are just adding axis labels and pulling the axes in a bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5730dbc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(fprs, tprs)\n",
    "plt.xlabel('False Positive Rate (i.e. 1 - Specificity)', fontsize=16)\n",
    "plt.ylabel('True Positive Rate (i.e. Sensitivity)', fontsize=16)\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "405db399",
   "metadata": {},
   "source": [
    "## Plot the PR Curve!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ea18ad0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(tprs, ppvs)\n",
    "plt.xlabel('Recall (i.e. Sensitivity, TPR)', fontsize=16)\n",
    "plt.ylabel('Precision (i.e. PPV)', fontsize=16)\n",
    "plt.xlim([-.01, 1.01])\n",
    "plt.ylim([-.01, 1.01])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "700ecd25",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
