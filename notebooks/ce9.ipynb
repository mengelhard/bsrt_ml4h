{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Exercise 9: Bag of Words Models\n",
    "\n",
    "**Please note that (optionally) this exercise may be completed in groups of 2 students.**\n",
    "\n",
    "---\n",
    "In this exercise, we'll convert sentences from different sections of medical abstracts (e.g. background, methods, etc) into bag of words feature vectors. In a subsequent exercise, we'll then use these feature vectors to develop and test a predictive model.\n",
    "\n",
    "Goals are as follows:\n",
    "\n",
    "- Further improve your understanding of count-based text features\n",
    "- Learn how to convert text data into features that can be used to develop a predictive model\n",
    "\n",
    "We'll begin by importing the usual libraries in addition to `requests`, which will help us load the dataset from url. Later on, we'll also import a new one, the **natural language toolkit (nltk)**, which will help us preprocess our text data.\n",
    "\n",
    "- numpy for efficient math operations\n",
    "- pandas for data and dataframe manipulations\n",
    "- matplotlib for visualization/plotting\n",
    "- requests to load data from url\n",
    "- **nltk for text pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We'll be working with the \"PubMed 200k RCT dataset\" dataset developed by Franck Dernoncourt. This dataset contains sentences from different sections of Pubmed abstracts along with labels indicating which section they're from. The sections are:\n",
    "\n",
    "- OBJECTIVE\n",
    "- BACKGROUND\n",
    "- METHODS\n",
    "- RESULTS\n",
    "- CONCLUSIONS\n",
    "\n",
    "Over the next few exercises, our goal will be to develop a classifier that assigns sentences to the correct label. This is not a very useful classifier, but shows that natural language processing is effective even for text with complex terminology, including clinical notes. The training, validation, and test data are found at the following addresses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/train.txt?raw=true'\n",
    "val_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/dev.txt?raw=true'\n",
    "test_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/test.txt?raw=true'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll begin by defining a function to read these data. Much like in previous exercises, **the details here are *not* important to our goals;** we just need the data. For now, we'll load only the training data (as `sentences` and `labels`), but in later exercises, we'll reuse these addresses and the function below to load the validation and test sets as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 180040 sentences in the training set\n",
      "There are 30212 sentences in the validation set\n",
      "There are 30135 sentences in the test set\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "def read_pubmed_rct(url):\n",
    "\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    \n",
    "    with requests.get(url) as r:\n",
    "        for line in r.iter_lines():\n",
    "            fields = line.decode('utf-8').strip().split('\\t')\n",
    "            if len(fields) == 2:\n",
    "                labels.append(fields[0])\n",
    "                sentences.append(fields[1])\n",
    "                \n",
    "    return sentences, labels\n",
    "\n",
    "sentences_train, y_train = read_pubmed_rct(train_url)\n",
    "print('There are %i sentences in the training set' % len(sentences_train))\n",
    "\n",
    "sentences_val, y_val = read_pubmed_rct(val_url)\n",
    "print('There are %i sentences in the validation set' % len(sentences_val))\n",
    "\n",
    "sentences_test, y_test = read_pubmed_rct(test_url)\n",
    "print('There are %i sentences in the test set' % len(sentences_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Natural Language Toolkit (NLTK) for text processing\n",
    "\n",
    "We can now import NLTK. We'll first make sure it's installed, since it's not part of the Anaconda base environment. We'll also import:\n",
    "- `word_tokenize`, which splits a sentence into a list of *tokens* (e.g. words, numbers, punctuation)\n",
    "- `stopwords`, a list of commonly used words that we can safely ignore when processing our text\n",
    "- `PorterStemmer`, which will convert words into stems, as described in the lecture and shown in an example below\n",
    "\n",
    "We'll also download lists of punctuation ('punkt') and stopwords ('stopwords'), then create `sw`, a set containing all the stopwords, and `ps`, an instance of `PorterStemmer` that we can apply to our words. **Before moving on, take a look at** `sw` **and try out** `word_tokenize` **on a few different sentences.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Users/mme/miniforge3/lib/python3.9/site-packages (3.6.5)\r\n",
      "Requirement already satisfied: joblib in /Users/mme/miniforge3/lib/python3.9/site-packages (from nltk) (1.0.1)\r\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/mme/miniforge3/lib/python3.9/site-packages (from nltk) (2021.10.23)\r\n",
      "Requirement already satisfied: click in /Users/mme/miniforge3/lib/python3.9/site-packages (from nltk) (8.0.3)\r\n",
      "Requirement already satisfied: tqdm in /Users/mme/miniforge3/lib/python3.9/site-packages (from nltk) (4.61.2)\r\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mme/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /Users/mme/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk\n",
    "import nltk\n",
    "\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "sw = set(stopwords.words('english'))\n",
    "ps = PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sentence):\n",
    "    return [\n",
    "        ps.stem(w.lower())\n",
    "        for w in word_tokenize(sentence)\n",
    "        if w.replace(\"'\", \"\", 1).isalpha() and (w not in sw)\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.2: Process all sentences\n",
    "\n",
    "You can now use a single list comprehension to apply `tokenize` to *all* of the sentences, resulting in a list of 180,040 stemmed, tokenized sentences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens_train = [tokenize(s) for s in sentences_train]\n",
    "tokens_val = [tokenize(s) for s in sentences_val]\n",
    "tokens_test = [tokenize(s) for s in sentences_test]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.3: Create your vocabulary\n",
    "\n",
    "We're now ready to create our vocabulary using the approach described in the [bag of words lecture](https://github.com/mengelhard/bsrt_ml4h/blob/master/lectures/al10.pdf). You'll need to complete the following steps:\n",
    "- Put the stemmed tokens from *all* sentences together in a single list or array. This can be done with a list comprehension or `np.concatenate`.\n",
    "- Count the number of occurrences of each distinct token. This can be done with `np.unique` (use `return_counts=True`) or `pd.value_counts`.\n",
    "- Remove those that occur fewer than 50 times. This can be done using boolean indexing: if we have the arrays `words` and `word_counts`, for example, we can write `vocabulary = words[word_counts >= 50]`. Later on, we'll explore how making this number larger or smaller affects model performance.\n",
    "\n",
    "The resulting list (or array) is your vocabulary, which defines the features for our bag of words model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 3986 words in our vocabulary\n"
     ]
    }
   ],
   "source": [
    "vcs = pd.value_counts([w for s in tokens_train for w in s])\n",
    "vocabulary = vcs.index.values[vcs >= 50]\n",
    "print('There are %i words in our vocabulary' % len(vocabulary))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define a function to create features\n",
    "\n",
    "Finally, we can use (a) the vocabulary, and (b) our list of stemmed, tokenized sentences to create numeric features corresponding to each sentence. The block below defines a function `create_features` and shows how it can be applied to a sample list of tokenized sentences along with a sample vocabulary. **You do not need to make changes to this block, but please take a look at the code and verify that it is creating feature vectors using the approach described in our lecture.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(tokenized_sentences, vocabulary):\n",
    "    \n",
    "    vocab_dict = {v:i for i, v in enumerate(vocabulary)}\n",
    "    \n",
    "    features = np.zeros((len(tokenized_sentences), len(vocabulary)))\n",
    "    \n",
    "    for i, tokenized_sentence in enumerate(tokenized_sentences):\n",
    "        for word in tokenized_sentence:\n",
    "            if word in vocabulary:\n",
    "                features[i, vocab_dict[word]] += 1\n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 8.4: Create the feature vectors\n",
    "\n",
    "*Your* list and vocabulary have been stemmed, so they'll look different than those in the example above. What's important is that the format of tokens in the vocabulary matches the format in the tokenized sentences, which should be the case if you've followed the steps outlined above.\n",
    "\n",
    "In the block below, apply `create_features` to your tokenized sentence list and vocabulary to create `x_train`, which we'll use to train a predictive model in our next computational exercise. This may take a few minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = create_features(tokens_train, vocabulary)\n",
    "x_val = create_features(tokens_val, vocabulary)\n",
    "x_test = create_features(tokens_test, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(180040, 3986)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.1: Create a Logistic Regression Model\n",
    "\n",
    "- train it on the training set\n",
    "- evaluate it on the validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['OBJECTIVE',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'METHODS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS',\n",
       " 'RESULTS']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mme/miniforge3/lib/python3.9/site-packages/sklearn/linear_model/_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 75.9\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_model = LogisticRegression().fit(x_train, y_train)\n",
    "\n",
    "print('The accuracy is %.1f' % (100 * np.mean(lr_model.predict(x_val) == y_val)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.2: Figure out which words are most important"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 9.3: Tune the model and evaluate it on the test set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Once you've completed these exercises, please turn in the assignment as follows:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "- download your notebook as html (see File > Download as > HTML (.html))\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS\n",
    "\n",
    "If you're using Google Colab:\n",
    "- download your notebook as .ipynb (see File > Download > Download .ipynb)\n",
    "- if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
