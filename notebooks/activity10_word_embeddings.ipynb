{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Activity 10: Exploring Word Embeddings\n",
    "\n",
    "---\n",
    "In this activity, we'll load word embeddings and take a look at a few of their properties. This will prepare us to create a predictive model based on word embeddings in a later computational exercise. Goals of the activity are as follows:\n",
    "\n",
    "- Learn how to load word embeddings\n",
    "- Calculate distances between words\n",
    "- Find the word(s) that are most similar to a given word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the Embeddings\n",
    "\n",
    "We'll begin by loading our word vectors/embeddings. We're going to use 300-dimensional embeddings from the [Stanford NLP group (i.e. GloVe)](https://nlp.stanford.edu/projects/glove/). However, we won't be loading all of the >1 million words available directly from the Stanford site. Instead, we'll be working from a much smaller file that contains only those words that are common in PubMed abstracts (from the PubMed 200k RCT dataset). No need to worry about how we're loading the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import requests\n",
    "import shutil\n",
    "\n",
    "response = requests.get(\n",
    "    'https://github.com/mengelhard/mmci_applied_ds/raw/master/data/glove/ce3_glove.npy',\n",
    "    stream=True)\n",
    "\n",
    "with open('glove.npy', 'wb') as fin:\n",
    "    shutil.copyfileobj(response.raw, fin)\n",
    "\n",
    "glove_dict = np.load('glove.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`glove_dict` is a Python *dictionary*, which maps unique *keys* to *values*. In our case, the keys are words, and the values are the associated word vectors. If we want to know if a given word is one of the keys in our dictionary, we can write `'word' in glove_dict.keys()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'word' in glove_dict.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, if we want to look up the vector for `'word'`, we can write `glove_dict['word']`, which will give us a 300-dimensional vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_dict['word'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Look up the embeddings for a few medical terms\n",
    "\n",
    "How rich is the vocabulary we get from our word vectors? In this portion of the activity, you should:\n",
    "- check whether a few medical terms are found in `glove_dict`\n",
    "- retrieve the word vectors associated with these terms\n",
    "\n",
    "Note that our dictionary only contains words found in **both** our PubMed and in the original GloVe dictionary, which contains words from Wikipedia. You'll find some medical terms but not others, and you won't find any legal terms, for instance, because they're not found in PubMed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Calculate the similarity between words\n",
    "\n",
    "Using our word vectors, we can evaluate the similarity between pairs of words in our dictionary by taking the inner product (also called the *dot product* or *cosine similarity*), which gives us the cosine of the angle between the two word vectors (since these vectors have been unit-normalized).\n",
    "\n",
    "- When the angle $\\theta$ is close to zero, $\\cos(\\theta)$ will be close to 1.\n",
    "- As $\\theta$ gets larger, $\\cos(\\theta)$ gets smaller (and, in some cases, negative).\n",
    "\n",
    "Supposing 'word1' and 'word2' are both in `glove_dict`, we can calculate their dot product as `np.sum(glove_dict['word1'] * glove_dict['word2'])`.\n",
    "\n",
    "In this part of the exercise, you should calculate the word similarity between:\n",
    "1. a few pairs of words you'd expect to be closely related\n",
    "2. a few pairs of words you'd expect to be unrelated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Words most similar to a given word\n",
    "\n",
    "Last but not least, we can find the words in our dictionary that are *most similar* to a given word by iterating over the dictionary. We can do this by:\n",
    "1. Iterating over the keys (e.g. `for key in glove_dict.keys()`)\n",
    "2. Iterating over key, value pairs with `.items()` (e.g. `for key, value in glove_dict.items()`)\n",
    "\n",
    "Either way, we want to:\n",
    "- calculate the similarity between the word of interest and each other word in the dictionary\n",
    "- sort the words by their similarity to the word of interest\n",
    "\n",
    "`pandas` gives us an easy way to do the latter: we can create a series, then sort it by the similarity values. The block below provides a possible implementation along with an example. Please either (a) write your own implementation, or (b) go over this implementation carefully to make sure you understand how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "lisinopril      1.000000\n",
       "ramipril        0.546203\n",
       "captopril       0.541375\n",
       "atenolol        0.506404\n",
       "infliximab      0.503944\n",
       "hydralazine     0.501791\n",
       "donepezil       0.483091\n",
       "verapamil       0.482338\n",
       "budesonide      0.481172\n",
       "enalapril       0.479861\n",
       "fluvoxamine     0.476956\n",
       "clonidine       0.474255\n",
       "propranolol     0.473654\n",
       "exenatide       0.472517\n",
       "ppis            0.464988\n",
       "salmeterol      0.464125\n",
       "pantoprazole    0.463856\n",
       "etanercept      0.463551\n",
       "atomoxetine     0.461104\n",
       "temozolomide    0.457288\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def similarity_to(wordvec):\n",
    "    return pd.Series({k: np.sum(wordvec * v) for k, v in glove_dict.items()}).sort_values(ascending=False)\n",
    "\n",
    "similarity_to(glove_dict['lisinopril']).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in this part of the exercise, you should:\n",
    "1. find words that are most similar to a few medical terms of interest and comment on whether / how much you agree with the ranking\n",
    "2. see which words are *least* similar to those words and comment on their significance or lack thereof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "### YOUR CODE HERE ###\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
