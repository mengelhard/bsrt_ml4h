{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ce7.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.12"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qqj6lIUuT75C"
      },
      "source": [
        "# Computational Exercise 7: Transfer Learning\n",
        "\n",
        "**It is recommended that you complete this exercise in Google Colab:**\n",
        "- otherwise you may encounter errors\n",
        "- this will allow you to access free GPU resources\n",
        "\n",
        "---\n",
        "In this exercise, we'll use transfer learning to train the convolutional neural network (CNN) used in our earlier activity, Inception v3, to classify images of flowers. A very similar process was used by Esteva et al. to classify skin lesions.\n",
        "\n",
        "**This exercise should be completed using a GPU runtime in Colab.**\n",
        "\n",
        "As in that activity, we'll be importing `tensorflow` as well as `tensorflow_hub`, which will help us load an Inception v3 model that has already been trained (i.e. *pre*-trained) on ImageNet."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wdHPqDXqBKPL"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_hub as hub"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zul0DOXnWDgW"
      },
      "source": [
        "Let's begin by using `tensorflow_hub` to load Inception v3. I found this URL by searching available models on [TensorFlow Hub](https://tfhub.dev). This step may take a minute or two. Once it's complete, we'll have the pre-trained Inception v3 model as a [SavedModel object](https://www.tensorflow.org/guide/saved_model) called `inception_v3`.\n",
        "\n",
        "**Important Note**: This is a *different* version of Inception v3 compared to the one we used in an earlier activity. In this version, the final layer (i.e. the *classification head*) has been removed."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtDXkJUkCIOU",
        "scrolled": false
      },
      "source": [
        "INCEPTION_V3_URL = 'https://tfhub.dev/google/tf2-preview/inception_v3/feature_vector/4'\n",
        "INPUT_SIZE = 299\n",
        "inception_v3 = hub.load(INCEPTION_V3_URL)"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TS7aJdTvnP_X"
      },
      "source": [
        "## Part 1: Identifying Flowers\n",
        "\n",
        "In the first half of this activity, we'll re-train the final layer of Inception v3 to identify several types of flowers. The next few blocks use code from a [Tensorflow Transfer Learning tutorial](https://www.tensorflow.org/tutorials/images/transfer_learning_with_hub). We begin by downloading the dataset using a function provided by Tensorflow."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iHeXWq_FFqs4"
      },
      "source": [
        "flowers_root = tf.keras.utils.get_file(\n",
        "  'flower_photos',\n",
        "  'https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz',\n",
        "   untar=True)"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UFjPpNwWoZUG"
      },
      "source": [
        "We can now build our dataset directly from the downloaded files, again using a utility provided by Tensorflow. This utility provides a data pipeline that will:\n",
        "- Load all images in batches during training along with their associated labels. The labels are determined by the directory structure in the archive we downloaded.\n",
        "- Resize all images to (299, 299) as required for Inception v3.\n",
        "- Convert the images and labels to Tensorflow tensors ready to be fed through a Tensorflow graph.\n",
        "- Divide the data into training and test sets.\n",
        "\n",
        "We'll also need to use a Rescaling layer to make sure the pixels in our images will range from 0 to 1 rather than 0 to 255.\n",
        "\n",
        "**The details of the code in this block are not important, but it is important that you understand why these steps above are needed.**\n",
        "\n",
        "Finally, we'll add a few lines to tell Tensorflow to pre-fetch images into memory, which will speed up the training process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y1cm93gfFqs5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e348917f-8cee-49ec-e2ee-91a749ad59b9"
      },
      "source": [
        "FLOWER_CLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips']\n",
        "\n",
        "train_flowers = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  flowers_root,\n",
        "  validation_split=0.2,\n",
        "  subset=\"training\",\n",
        "  seed=123,\n",
        "  class_names=FLOWER_CLASSES,\n",
        "  image_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "  batch_size=32\n",
        ")\n",
        "\n",
        "test_flowers = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "  flowers_root,\n",
        "  validation_split=0.2,\n",
        "  subset=\"validation\",\n",
        "  seed=123,\n",
        "  class_names=FLOWER_CLASSES,\n",
        "  image_size=(INPUT_SIZE, INPUT_SIZE),\n",
        "  batch_size=32\n",
        ")\n",
        "\n",
        "normalization_layer = tf.keras.layers.Rescaling(1./255)\n",
        "train_flowers = train_flowers.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
        "test_flowers = test_flowers.map(lambda x, y: (normalization_layer(x), y)) # Where x—images, y—labels.\n",
        "\n",
        "AUTOTUNE = tf.data.AUTOTUNE\n",
        "train_flowers = train_flowers.cache().prefetch(buffer_size=AUTOTUNE)\n",
        "test_flowers = test_flowers.cache().prefetch(buffer_size=AUTOTUNE)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 3670 files belonging to 5 classes.\n",
            "Using 2936 files for training.\n",
            "Found 3670 files belonging to 5 classes.\n",
            "Using 734 files for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zTo5dol9sn9X"
      },
      "source": [
        "## Exercise 8.1: Visualize the Flowers Dataset\n",
        "\n",
        "Similar to previous exercises, we can use `plt.imshow` to plot images in a batch generated by our `train_flowers` Dataset. In the following block, you should plot and label these images."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TC_RYB7EFqs5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "outputId": "3316079d-b707-43ea-c043-489ceedabbf8"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
        "\n",
        "# get a single batch\n",
        "images, labels = next(iter(train_flowers))\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  ## ADD CODE TO PLOT THE IMAGE ##\n",
        "\n",
        "\n",
        "  ## ADD CODE TO TITLE THE IMAGE WITH THE CORRESPONDING LABEL ##\n",
        "\n",
        "  \n",
        "  # turn off the axes to make the images look nice\n",
        "  ax[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o9yzSm1lv7Xb"
      },
      "source": [
        "We're now ready to create our prediction model following the steps described in our [medical image analysis lecture](https://github.com/mengelhard/bsrt_ml4h/blob/master/lectures/ll3.pdf):\n",
        "- Start with our Inception v3 feature extractor. The classification head (i.e. final layer) has already been removed, but we do need to convert this from a Tensorflow Hub model to a Layer object that can be incorporated in our new, flower prediction model.\n",
        "- Note that we're saying `trainable=False`, which tells Tensorflow that the existing Inception v3 layers are *not* trainable. In other words, we will be training only our new flower prediction head, not fine-tuning Inception v3. If we *did* want to fine-tune, we'd need much more images, and more time. However, in terms of the coding required at this stage, all we'd need to do is pass `trainable=True` instead.\n",
        "- Add a Dense (i.e. fully-connected) layer that will predict the image label (i.e. flower type) based on the extracted features. We will be learning the weights in this new layer.\n",
        "\n",
        "We can now use `model.summary()` to see a description of our model. Note that our single `keras_layer` is actually the entire Inception v3 CNN, which is why it contains so many parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qqHQ30SDHQSf",
        "outputId": "34085830-03d2-4e9d-df1a-41dd5201ee66"
      },
      "source": [
        "feature_extractor = hub.KerasLayer(\n",
        "  inception_v3,\n",
        "  input_shape=(INPUT_SIZE, INPUT_SIZE, 3),\n",
        "  trainable=False)\n",
        "\n",
        "# replace the classification head\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  feature_extractor,\n",
        "  tf.keras.layers.Dense(len(FLOWER_CLASSES))\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer (KerasLayer)     (None, 2048)              21802784  \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (None, 5)                 10245     \n",
            "=================================================================\n",
            "Total params: 21,813,029\n",
            "Trainable params: 10,245\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PJLlY24ezfhH"
      },
      "source": [
        "To begin training, we'll first need to define our loss, and our optimizer, just as we did in our previous computational exercise."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMEWOgXVHQdK"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # multi-class cross-entropy loss\n",
        "optimizer = tf.keras.optimizers.Adam() # modified stochastic gradient descent optimizer"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpVNPNg5mNVM"
      },
      "source": [
        "## Exercise 8.2: Train the Flowers Model and Evaluate Performance\n",
        "\n",
        "We're now ready to train our model, which we can do with the following block. Again, this is identical to code we used in the previous exercise. However, this time you should **make the following adjustments**:\n",
        "- Try training for more epochs\n",
        "- Add code to monitor accuracy on the training set and test set after each epoch\n",
        "- After the training process has completed, plot training and test performance by epoch. Is your model overfitting?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCaArLeEHQj6",
        "outputId": "44c63d6a-09e9-4cc1-b8b6-dcf8cd6afc1b"
      },
      "source": [
        "EPOCHS = 3\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "    for images, labels in train_flowers:\n",
        "    \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(images)\n",
        "            loss = loss_object(labels, predictions)\n",
        "    \n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    print('Completed epoch', epoch)"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed epoch 0\n",
            "Completed epoch 1\n",
            "Completed epoch 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d_v0m6uiz9UI"
      },
      "source": [
        "## Exercise 8.3: Make Predictions and Inspect Them\n",
        "\n",
        "We're done training! We've already taken a look at the accuracy, but let's also inspect a few images in the test set. In the following block, you should plot at least 5 test images along with (a) the predicted label, and (b) the true label."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "KzT_h_eaHQoU",
        "outputId": "e0e4ae6f-3d86-4932-806a-59ffa3395dc1"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
        "\n",
        "images, labels = next(iter(test_flowers))\n",
        "\n",
        "## ADD CODE TO PREDICT THE LABELS FOR THIS BATCH OF TEST IMAGES ##\n",
        "\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  ## ADD CODE TO PLOT THE IMAGE ##\n",
        "\n",
        "\n",
        "  ## ADD CODE TO TITLE THE IMAGE WITH THE PREDICTED AND TRUE LABELS ##\n",
        "  \n",
        "  \n",
        "  # turn off the axes to make the images look nicer\n",
        "  ax[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xhoN78yqdQOd"
      },
      "source": [
        "## Part 2: Mammograms\n",
        "\n",
        "In the second half of this activity, we'll re-train the final layer of Inception v3 to identify types of breast tissue from [the mini-MIAS database of mammograms](http://peipa.essex.ac.uk/info/mias.html). This dataset is very small, which makes it easy to work with in a short amount of time. While we're unlikely to get good accuracy with a dataset of this size, the same process could be used to obtain good performance on a larger dataset. We'll begin by downloading the data the same way we downloaded the flowers dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L5c-jvu03-7O"
      },
      "source": [
        "mias_root = tf.keras.utils.get_file(\n",
        "  origin='http://peipa.essex.ac.uk/pix/mias/all-mias.tar.gz',\n",
        "  untar=True)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3werxg12CRO"
      },
      "source": [
        "We can now build our dataset from the downloaded files. This time, we'll build the dataset manually instead of using a Tensorflow function. This will make it easier to see all the individual steps, but will use more RAM than we did before, because all the images will be stored in memory. Steps include:\n",
        "- Load all images in the directory we downloaded using `cv2`.\n",
        "- Resize all images to (299, 299) and rescale their pixels to range from 0 to 1, as required for Inception v3.\n",
        "- Convert the images and labels to Tensorflow tensors ready to be fed through a Tensorflow graph.\n",
        "- Use pandas to read a table containing information corresponding to each image, including the tissue type and severity of any abnormalities. We will be predicting tissue type rather than severity, because the latter only exists for images that contain abnormalities.\n",
        "- Divide the data into training and test sets.\n",
        "\n",
        "**The details of the code in this block are not important, but it is important that you understand why these steps above are needed.**\n",
        "\n",
        "After running this block, you may want to take a look at `mias_df`, which contains information about each image, as well as the shape of `mias_images`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zad_FQFq5lNu",
        "outputId": "910ba8a7-ee74-4f19-dc73-c5073200268c"
      },
      "source": [
        "import cv2\n",
        "\n",
        "def read_image(fn, size=(INPUT_SIZE, INPUT_SIZE)):\n",
        "  img = cv2.imread(fn)[:, :, ::-1]\n",
        "  img = cv2.resize(img, size)\n",
        "  return img / 255.\n",
        "\n",
        "mias_df = pd.read_table(\n",
        "    os.path.join(os.path.dirname(mias_root), 'Info.txt'),\n",
        "    sep=' ',\n",
        "    skiprows=101,\n",
        "    skipfooter=2,\n",
        "    header=None,\n",
        "    names=['id', 'tissue', 'type', 'severity', 'x_coord', 'y_coord', 'radius']\n",
        ")[['id', 'tissue', 'type']].drop_duplicates().sample(frac=1., random_state=2021)\n",
        "\n",
        "mias_filenames = os.listdir(os.path.dirname(mias_root))\n",
        "mias_ids = [fn.split('.')[0] for fn in mias_filenames]\n",
        "mias_df = mias_df[mias_df['id'].isin(mias_ids)]\n",
        "\n",
        "mias_labels, mias_classes = pd.factorize(mias_df['tissue'])\n",
        "mias_images = np.array([read_image(os.path.join(os.path.dirname(mias_root), str(id) + '.pgm')) for id in mias_df['id']])\n",
        "\n",
        "train_mias = tf.data.Dataset.from_tensor_slices((mias_images[:(23 * 12)], mias_labels[:(23 * 12)])).batch(23)\n",
        "test_mias = tf.data.Dataset.from_tensor_slices((mias_images[(23 * 12):], mias_labels[(23 * 12):])).batch(23)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/pandas/io/parsers.py:767: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support skipfooter; you can avoid this warning by specifying engine='python'.\n",
            "  return read_csv(**locals())\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQnx8gjW4Y6k"
      },
      "source": [
        "## Exercise 8.4: Visualize the mini-MIAS Dataset\n",
        "\n",
        "This time, even though we did create Tensorflow Datasets to use in training (`train_mias` and `test_mias`), we can also access the images and labels directly in `mias_images` and `mias_labels`, and `mias_classes` tells us the tissue type corresponding to each label. In the following block, you should plot and label at least 5 images from mini-MIAS."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        },
        "id": "OhrO7loJ-Vlm",
        "outputId": "e003c3af-05af-46a5-c586-8afd13b4d37c"
      },
      "source": [
        "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
        "\n",
        "for i in range(5):\n",
        "\n",
        "  ## ADD CODE TO PLOT THE IMAGE ##\n",
        "\n",
        "\n",
        "  ## ADD CODE TO TITLE THE IMAGE WITH THE CORRESPONDING LABEL ##\n",
        "  \n",
        "  \n",
        "  # turn off the axes to make the images look nice\n",
        "  ax[i].axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1080x216 with 5 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OOOb_auH5oKk"
      },
      "source": [
        "Once again, we're ready to create our prediction model following the steps described in our [medical image analysis lecture](https://github.com/mengelhard/bsrt_ml4h/blob/master/lectures/ll3.pdf):\n",
        "- Start with our Inception v3 feature extractor. The classification head (i.e. final layer) has already been removed, but we do need to convert this from a Tensorflow Hub model to a Layer object that can be incorporated in our new, flower prediction model.\n",
        "- We'll stick with `trainable=False`. It may be interesting to try `trainable=True`, but it's unlikely to work well with such a small dataset. If we had a large enough dataset to support this, we'd need a lot more time.\n",
        "- Add a Dense (i.e. fully-connected) layer that will predict the image label (i.e. tissue type) based on the extracted features. We will be learning the weights in this new layer.\n",
        "\n",
        "We can now use `model.summary()` to see a description of our model. Again, our single `keras_layer` is actually the entire Inception v3 CNN, which is why it contains so many parameters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uK4sLDSePpdp",
        "outputId": "041f18c9-6a29-4672-cfab-48f6ea813699"
      },
      "source": [
        "feature_extractor = hub.KerasLayer(\n",
        "  inception_v3,\n",
        "  input_shape=(INPUT_SIZE, INPUT_SIZE, 3),\n",
        "  trainable=False)\n",
        "\n",
        "# replace the classification head\n",
        "\n",
        "model = tf.keras.Sequential([\n",
        "  feature_extractor,\n",
        "  tf.keras.layers.Dense(len(mias_classes))\n",
        "])\n",
        "\n",
        "model.summary()"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "keras_layer_1 (KerasLayer)   (None, 2048)              21802784  \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 3)                 6147      \n",
            "=================================================================\n",
            "Total params: 21,808,931\n",
            "Trainable params: 6,147\n",
            "Non-trainable params: 21,802,784\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AKqij8dy6IHi"
      },
      "source": [
        "We'll use the same loss and optimizer as before."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NlHK52JcT9m1"
      },
      "source": [
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # multi-class cross-entropy loss\n",
        "optimizer = tf.keras.optimizers.Adam() # modified stochastic gradient descent optimizer"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS4jaJmx6ZF8"
      },
      "source": [
        "## Exercise 8.5: Train the MIAS Model and Evaluate Performance\n",
        "\n",
        "Now we're ready to start training! You should modify the code below the same way you did before:\n",
        "- Evaluate accuracy on the training set and test set after each epoch\n",
        "- Additionally, it may be helpful to evaluate performance on the test set before we start training so we know what our starting point is."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ATdipKNNRIBS",
        "outputId": "a4689e2f-b97b-463b-c68e-7c172685864c"
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "  \n",
        "    for images, labels in train_mias:\n",
        "    \n",
        "        with tf.GradientTape() as tape:\n",
        "            predictions = model(images)\n",
        "            loss = loss_object(labels, predictions)\n",
        "    \n",
        "        gradients = tape.gradient(loss, model.trainable_variables)\n",
        "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "        \n",
        "    print('Completed epoch', epoch)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Completed epoch 0\n",
            "Completed epoch 1\n",
            "Completed epoch 2\n",
            "Completed epoch 3\n",
            "Completed epoch 4\n",
            "Completed epoch 5\n",
            "Completed epoch 6\n",
            "Completed epoch 7\n",
            "Completed epoch 8\n",
            "Completed epoch 9\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9C5x60rz7AGK"
      },
      "source": [
        "When training a model like this, in addition to the fact that you'll typically be working with a much larger dataset, there are a few additional factors to consider, including:\n",
        "- The depth to which you'll fine-tune. To adjust this, we'd need code to set individual layers within Inception v3 to `trainable=True` or `trainable=False` rather than Inception v3 as a whole.\n",
        "-  Data augmentation, which typically includes random crops, rotations, and other distortions.\n",
        "\n",
        "Still, this code provides the general road map you'd need to make predictions on your own dataset of medical images.\n",
        "\n",
        "## Once you've completed these exercises, please turn in the assignment as follows:\n",
        "\n",
        "If you're using Anaconda on your local machine:\n",
        "- download your notebook as html (see File > Download as > HTML (.html))\n",
        "- .zip the file (i.e. place it in a .zip archive)\n",
        "- submit the .zip file in Talent LMS\n",
        "\n",
        "If you're using Google Colab:\n",
        "- download your notebook as .ipynb (see File > Download > Download .ipynb)\n",
        "- if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
        "- .zip the file (i.e. place it in a .zip archive)\n",
        "- submit the .zip file in Talent LMS"
      ]
    }
  ]
}