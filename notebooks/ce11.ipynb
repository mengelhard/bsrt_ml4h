{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d98d5e2",
   "metadata": {},
   "source": [
    "# Computational Exercise 11: Human Activity Recognition\n",
    "\n",
    "**Please note that (optionally) this assignment may be completed in groups of 2 students.**\n",
    "\n",
    "---\n",
    "In this exercise, we'll be working with the [Human Activity Recognition Using Smartphones Data Set](https://archive.ics.uci.edu/ml/datasets/human+activity+recognition+using+smartphones), which is exactly what it sounds like. As described on [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php) site:\n",
    "- Experiments were carried out with a group of 30 volunteers within an age bracket of 19-48 years\n",
    "- Each person performed six activities (WALKING, WALKING_UPSTAIRS, WALKING_DOWNSTAIRS, SITTING, STANDING, LAYING) wearing a smartphone (Samsung Galaxy S II) on the waist\n",
    "- 3-axial linear acceleration and 3-axial angular velocity were captured at a constant rate of 50Hz with the phone's embedded accelerometer and gyroscope\n",
    "- The obtained dataset has been randomly partitioned into two sets, where 70% of the volunteers were selected for generating the training data and 30% the test data\n",
    "\n",
    "For each record, we have:\n",
    "- Triaxial (i.e. x-, y-, and z-direction) acceleration from the accelerometer (total acceleration) and the estimated body acceleration\n",
    "- Triaxial Angular velocity from the gyroscope\n",
    "- A label indicating the corresponding activity\n",
    "- There is also a 561-feature vector with extracted time and frequency domain variables, but we will not be using it in this exercise\n",
    "\n",
    "Goals are as follows:\n",
    "\n",
    "- Describe and visualize the dataset\n",
    "- Extract summary statistics for each record\n",
    "- Predict activities using the extracted summary statistics\n",
    "- Predict activities by applying a simple RNN to the raw data (i.e. without extracting summary statistics)\n",
    "\n",
    "We'll begin by importing the usual libraries including `tensorflow`, which we'll use to create and train the RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "56b17046",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37bb441c",
   "metadata": {},
   "source": [
    "## Download the dataset\n",
    "\n",
    "First, we'll need to download the dataset. The block below will download it into a folder named 'HAR' in your current working directory. By default, this is the directory where this notebook is located."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4525b659",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from io import BytesIO\n",
    "from urllib.request import urlopen\n",
    "from zipfile import ZipFile\n",
    "\n",
    "zipurl = \"https://archive.ics.uci.edu/ml/machine-learning-databases/00240/UCI HAR Dataset.zip\".replace(\" \", \"%20\")\n",
    "\n",
    "with urlopen(zipurl) as zipresp:\n",
    "    with ZipFile(BytesIO(zipresp.read())) as zfile:\n",
    "        zfile.extractall('HAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2563f4e1",
   "metadata": {},
   "source": [
    "## Load the dataset\n",
    "\n",
    "We can now load each of the 9 inertial signals described above along with the corresponding activity labels. The details of this block reflect the organization of this particular dataset and are less important than describing and visualizing the dataset, which you'll do in exercise 11.1 below. We'll also go ahead and standardize the data in this block, which will be important when training our RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "48d1d070",
   "metadata": {},
   "outputs": [],
   "source": [
    "### LIST THE 9 AVAILABLE INERTIAL SIGNALS ###\n",
    "### THESE CORRESPOND TO SPECIFIC FILES WE'LL LOAD FROM THE DATA DIRECTORY ###\n",
    "\n",
    "inertial_signals = [\n",
    "    'body_acc_x',\n",
    "    'body_acc_y',\n",
    "    'body_acc_z',\n",
    "    'total_acc_x',\n",
    "    'total_acc_y',\n",
    "    'total_acc_z',\n",
    "    'body_gyro_x',\n",
    "    'body_gyro_y',\n",
    "    'body_gyro_z'\n",
    "]\n",
    "\n",
    "### LOAD THE DATA ###\n",
    "\n",
    "x_train = np.concatenate([\n",
    "    pd.read_table(\n",
    "        os.path.join(\n",
    "            'HAR/UCI HAR Dataset/train/Inertial Signals',\n",
    "            sig + '_train.txt'),\n",
    "        header=None, sep='\\s+').values[:, :, np.newaxis]\n",
    "    for sig in inertial_signals], axis=2)\n",
    "\n",
    "x_test = np.concatenate([\n",
    "    pd.read_table(\n",
    "        os.path.join(\n",
    "            'HAR/UCI HAR Dataset/test/Inertial Signals',\n",
    "            sig + '_test.txt'),\n",
    "        header=None, sep='\\s+').values[:, :, np.newaxis]\n",
    "    for sig in inertial_signals], axis=2)\n",
    "\n",
    "y_train = pd.read_table(\n",
    "    'HAR/UCI HAR Dataset/train/y_train.txt',\n",
    "    header=None, squeeze=True\n",
    ").values - 1\n",
    "\n",
    "y_test = pd.read_table(\n",
    "    'HAR/UCI HAR Dataset/test/y_test.txt',\n",
    "    header=None, squeeze=True\n",
    ").values - 1\n",
    "\n",
    "### STANDARDIZE THE DATA ###\n",
    "\n",
    "x_mean = np.mean(x_train, axis=(0, 1))\n",
    "x_std = np.std(x_train, axis=(0, 1))\n",
    "\n",
    "x_train = (x_train - x_mean) / x_std\n",
    "x_test = (x_test - x_mean) / x_std"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01c98b32",
   "metadata": {},
   "source": [
    "## Exercise 11.1: Exploring the data\n",
    "\n",
    "In as many blocks as needed, explore the dataset, including:\n",
    "- Determine the shape of `x_train`, `x_test`, `y_train`, and `y_test`\n",
    "- Count the number of each type of label in `y_train` and `y_test`\n",
    "- Plot (using `plt.plot`) at least one inertial signal over time for at least one example. To do this, note that in `x_train`, the different examples are stacked along axis 0, time varies along axis 1, and the inertial signal varies along axis 2 (e.g. rotation and acceleration in each direction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9cfe472c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DETERMINE THE SHAPE OF THE DATA ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "73876d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "### COUNT AND/OR PLOT THE NUMBER OF EACH LABEL (i.e. activity) ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "deef8cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### PLOT AT LEAST ONE INERTIAL SIGNAL OVER TIME FOR AT LEAST ONE EXAMPLE ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89bc6095",
   "metadata": {},
   "source": [
    "## Exercise 11.2: Extract features\n",
    "\n",
    "As we've discussed in class, the simplest way to build a predictive model from repeated measures data is to calculate a limited set of summary statistics (e.g. maximum, minimum, mean) for each measure. This gives you a fixed length vector of length $M\\times L$, where $M$ is the number of repeated measures, and $L$ is the number of summary statistics. The block below shows how to construct this vector using just two summary statistics, the max and mean. \n",
    "\n",
    "In this block, you should:\n",
    "- Select at least two additional summary statistics to calculate\n",
    "- Following the example below, calculate them for each of your 9 inertial signals and stack all of the summary statistics together\n",
    "- Inspect the shape of the resulting features, noting that the first dimension (axis 0) should be unchanged compared to `x_train` and `x_test` from exercise 11.1 above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba4d6a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = np.concatenate(\n",
    "    [\n",
    "        x_train.max(axis=1),\n",
    "        x_train.min(axis=1)\n",
    "        # ADD AT LEAST TWO MORE FEATURES HERE. MAKE SURE YOU AGGREGATE OVER AXIS 1 ###\n",
    "    ],\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "x_test_features = np.concatenate(\n",
    "    [\n",
    "        x_test.max(axis=1),\n",
    "        x_test.min(axis=1)\n",
    "        # ADD THE SAME FEATURES AS YOU DID FOR THE TRAINING SET\n",
    "    ],\n",
    "    axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc88f791",
   "metadata": {},
   "source": [
    "## Exercise 11.3: Logistic regression\n",
    "\n",
    "Now, use this feature vector to build a model that predicts the activity labels. You may use any model you like. It may be instructive to see how choosing different summary statistics in exercise 11.2 affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3a434969",
   "metadata": {},
   "outputs": [],
   "source": [
    "### TRAIN A CLASSIFIER OF YOUR CHOICE (e.g. LogisticRegression) ON THE TRAINING SET (i.e. x_train_features) ###\n",
    "\n",
    "\n",
    "### EVALUATE ITS PERFORMANCE ON THE TEST SET (i.e. x_test_features) ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d6d19f2",
   "metadata": {},
   "source": [
    "## Exercise 11.4: LSTM\n",
    "\n",
    "Finally, let's see if we can improve performance using a recurrent neural network -- specifically, an LSTM. This code is almost identical to code you've seen before, and includes:\n",
    "- Loading specific layers we'll need from tensorflow, including a special LSTM layer\n",
    "- Defining our model -- in this case, a single LSTM block followed by a linear prediction layer -- and creating an instance of it\n",
    "- Defining our loss, which is the usual cross-entropy loss, as well as our optimizer\n",
    "- Converting our dataset to tensorflow tensors\n",
    "\n",
    "We'll then train for 10 epochs while evaluating accuracy on the training set and test set in each iteration.\n",
    "\n",
    "The code in these blocks is complete, and does not need to be modified before running the blocks. However, it may be interesting to see how changes to the model -- including the number of hidden units in the LSTM, which is currently set to 36 -- affects performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a153deed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, LSTM\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class MyLSTM(Model):\n",
    "    def __init__(self):\n",
    "        super(MyLSTM, self).__init__()\n",
    "        self.lstm = LSTM(36)\n",
    "        self.fc = Dense(6)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.lstm(x)\n",
    "        return self.fc(x)\n",
    "    \n",
    "model = MyLSTM()\n",
    "\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # multi-class cross-entropy loss\n",
    "optimizer = tf.keras.optimizers.Adam() # modified stochastic gradient descent optimizer\n",
    "\n",
    "# create tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.astype('float32'), y_train)).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test.astype('float32'), y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244a3d93",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "EPOCHS = 10\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_accuracy = []\n",
    "    test_accuracy = []\n",
    "  \n",
    "    for i, (x, y) in enumerate(train_ds):\n",
    "        \n",
    "        print('Running training batch %i of %i' % (i, len(train_ds)), end='\\r')\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted_logits = model(x)\n",
    "            loss = loss_object(y, predicted_logits)\n",
    "\n",
    "        y_pred = np.argmax(predicted_logits, axis=1)\n",
    "        batch_accuracy = np.mean(y_pred == y)\n",
    "        train_accuracy.append(batch_accuracy)\n",
    "    \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    for i, (x, y) in enumerate(test_ds):\n",
    "        \n",
    "        print('Running test batch %i of %i        ' % (i, len(test_ds)), end='\\r')\n",
    "    \n",
    "        predicted_logits = model(x)\n",
    "        y_pred = np.argmax(predicted_logits, axis=1)\n",
    "    \n",
    "        batch_accuracy = np.mean(y_pred == y)\n",
    "        test_accuracy.append(batch_accuracy)\n",
    "\n",
    "    train_accuracy = 100 * np.mean(train_accuracy)\n",
    "    test_accuracy = 100 * np.mean(test_accuracy)\n",
    "        \n",
    "    print('Epoch %i: train accuracy = %.1f%%, test accuracy = %.1f%%' % (\n",
    "        epoch, train_accuracy, test_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b45ea8d",
   "metadata": {},
   "source": [
    "### Once you've completed these exercises, please turn in the assignment as follows:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "- download your notebook as html (see File > Download as > HTML (.html))\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS\n",
    "\n",
    "If you're using Google Colab:\n",
    "- download your notebook as .ipynb (see File > Download > Download .ipynb)\n",
    "- if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369257c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
