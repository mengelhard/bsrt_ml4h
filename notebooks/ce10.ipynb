{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computational Exercise 10: A Simple Word Embedding Based Model\n",
    "\n",
    "---\n",
    "In this exercise, we'll continue working with the \"PubMed 200k RCT dataset\", but this time, we'll apply a model that's based on word *embeddings* rather than word *counts*.\n",
    "\n",
    "Goals of the exercise are as follows:\n",
    "\n",
    "- Load word embeddings and see how they can be used to convert text into a numeric array\n",
    "- Train and evaluate a VSWEM model, in which all word embeddings in a given document are aggregated to create a fixed-length feature vector\n",
    "- Train and evaluate a SWEM model, in which word embeddings are refined, then aggregated to create a fixed-length feature vector\n",
    "\n",
    "We'll need Tensorflow to train SWEM, so you may want to complete this exercise in Colab."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import requests, shutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next couple of blocks will load the dataset. We've seen this code several times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/train.txt?raw=true'\n",
    "val_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/dev.txt?raw=true'\n",
    "test_url = 'https://github.com/Franck-Dernoncourt/pubmed-rct/raw/master/PubMed_20k_RCT/test.txt?raw=true'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 180040 sentences in the training set\n",
      "There are 30212 sentences in the validation set\n",
      "There are 30135 sentences in the test set\n"
     ]
    }
   ],
   "source": [
    "def read_pubmed_rct(url):\n",
    "\n",
    "    labels = []\n",
    "    sentences = []\n",
    "    \n",
    "    with requests.get(url) as r:\n",
    "        for line in r.iter_lines():\n",
    "            fields = line.decode('utf-8').strip().split('\\t')\n",
    "            if len(fields) == 2:\n",
    "                labels.append(fields[0])\n",
    "                sentences.append(fields[1])\n",
    "                \n",
    "    return sentences, labels\n",
    "\n",
    "s_train, l_train = read_pubmed_rct(train_url)\n",
    "s_val, l_val = read_pubmed_rct(val_url)\n",
    "s_test, l_test = read_pubmed_rct(test_url)\n",
    "\n",
    "print('There are %i sentences in the training set' % len(s_train))\n",
    "print('There are %i sentences in the validation set' % len(s_val))\n",
    "print('There are %i sentences in the test set' % len(s_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.1: Sentence Length (in characters)\n",
    "\n",
    "Most of the time in natural language processing, we need to set a limit on how long are documents are allowed to be so that we can tell our model how big of an input it should expect. To get an initial sense of the length of our documents -- which in this case are sentences from PubMed abstracts -- let's plot a histogram of the sentence lengths (in characters) for the sentences in our training set. Note that the length of a single sentence can be determined with `len(sentence)`.\n",
    "\n",
    "In the code block below, you should:\n",
    "\n",
    "1. Calculate the length of all sentences in `s_train` using a list comprehension\n",
    "2. Use `plt.hist` to plot the lengths as a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### CALCULATE THE LENGTH OF EACH SENTENCE ###\n",
    "\n",
    "\n",
    "### CREATE A HISTOGRAM SHOWING SENTENCE LENGTH IN THE TRAINING SET ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings\n",
    "\n",
    "Now let's load our word vectors/embeddings. We'll use the same 300-dimensional embeddings from the Stanford NLP group (i.e. GloVe) that we used in Activity 10. No need to worry about how we're loading the file, but it's important to note that we can look up the vector for `'word'` by writing `glove_dict['word']`. For more details on this, please refer to Activity 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = requests.get(\n",
    "    'https://github.com/mengelhard/mmci_applied_ds/raw/master/data/glove/ce3_glove.npy',\n",
    "    stream=True)\n",
    "\n",
    "with open('glove.npy', 'wb') as fin:\n",
    "    shutil.copyfileobj(response.raw, fin)\n",
    "\n",
    "glove_dict = np.load('glove.npy', allow_pickle=True).item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.2: Sentence Length (in embeddings)\n",
    "\n",
    "We already know how long in characters our sentences tend to be, but the limit we'll need to place on the sentence length will be in terms of the maximimum number of *embeddings*.\n",
    "\n",
    "In the code block below, you should:\n",
    "1. Calculate the number of GloVe embeddings in each sentence using a list comprehension\n",
    "  - Use either `sentence.split()` or `word_tokenize` from `nltk` (see CE9) to tokenize the sentences\n",
    "  - Remember to convert all words to `.lower()` before checking whether a word is in `glove_dict.keys()` or attempting to retrieve the corresponding word vector\n",
    "  - Words should be counted only if they're in `glove_dict.keys()`\n",
    "2. Use `plt.hist` to plot the lengths as a histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### COUNT THE NUMBER OF GloVe EMBEDDINGS IN EACH SENTENCE ###\n",
    "\n",
    "\n",
    "### CREATE A HISTOGRAM SHOWING THESE COUNTS ACROSS THE TRAINING SET ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing the sentences for modeling\n",
    "\n",
    "The code below will convert each sentence in our dataset into a 40 by 300 array. First, all words in `glove_dict().keys()` are converted into word embeddings, each of which has length 300. Then, we stack all these embeddings together into an N by 300 array, where N is the number of embeddings we were able to retrieve from the words in the current sentence. Finally, if N is greater than 40, we'll use only the first 40 embeddings, and if N is less than 40, we'll pad with zeros to get a 40 by 300 array.\n",
    "\n",
    "The details of this block are not critical, but it may be helpful to think through these details carefully, particularly if you plan to work with your own NLP models. A few additional notes:\n",
    "- We are discarding sentences with 0 embeddings (i.e. N = 0)\n",
    "- We could choose `max_length` to be something other than 40. If you believe a different number might be better based the histogram you plotted in 10.2, please change this value. Note that this value is a hyperparameter of our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "y_dict = {'BACKGROUND': 0, 'OBJECTIVE': 1, 'METHODS': 2, 'RESULTS': 3, 'CONCLUSIONS': 4}\n",
    "\n",
    "\n",
    "def embed_sentence(s, max_length=40, embedding_dim=300):\n",
    "    \n",
    "    arr = [glove_dict[w.lower()] for w in s.split() if w.lower() in glove_dict.keys()]\n",
    "    arr = arr[:max_length]\n",
    "    \n",
    "    pad_length = max_length - len(arr)\n",
    "    pad = np.zeros((pad_length, embedding_dim))\n",
    "\n",
    "    return np.concatenate((arr, pad)) if len(arr) > 0 else None\n",
    "    \n",
    "    \n",
    "def process_sentences_and_labels(sentences, labels):\n",
    "    \n",
    "    embedded = [embed_sentence(s) for s in sentences]\n",
    "    x = np.stack([x for x in embedded if x is not None])\n",
    "    y = np.array([y_dict[l] for l, x in zip(labels, embedded) if x is not None])\n",
    "    \n",
    "    return x, y\n",
    "\n",
    "\n",
    "x_train, y_train = process_sentences_and_labels(s_train, l_train)\n",
    "x_val, y_val = process_sentences_and_labels(s_val, l_val)\n",
    "x_test, y_test = process_sentences_and_labels(s_test, l_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's quickly verify that the result came out as expected. `x_train`, `x_val`, and `x_test` should all have 3 dimensions. Take a moment to think about these arrays and what each dimension within them represents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((178988, 40, 300), (30024, 40, 300), (29946, 40, 300))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape, x_val.shape, x_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.3: A *very* simple word embedding based model (VSWEM)\n",
    "\n",
    "We're now ready to train our VSWEM model. In the block below, you should:\n",
    "1. Convert `x_train`, `x_val`, and `x_test` into 2-dimensional (rather than 3-dimensional) arrays by applying taking *either* the max or the average over `axis=1` (consider: why axis 1?)\n",
    "2. Train `LogisticRegression` or another model of your choice (e.g. `MLPClassifier`) on the training set, then evaluate its performance on the validation set\n",
    "3. (**optional**) Explore different models or variations, then evaluate your best performing model (as determined on the validation set) on the test set. One variation could be to concatenate the max and average into a single, 600-dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### USE A SUMMARY STATISTIC OF YOUR CHOICE TO AGGREGATE ACROSS AXIS 1 ###\n",
    "\n",
    "\n",
    "### TRAIN A MODEL OF YOUR CHOICE ON THE TRAINING SET ###\n",
    "\n",
    "\n",
    "### EVALUATE PERFORMANCE ON THE VALIDATION SET ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 10.4: A simple word embedding based model (SWEM)\n",
    "\n",
    "VSWEM is simple, but it doesn't work very well. We'll get much better performance by adding a fully connected layer that *refines* each word embedding before pooling them all together. To do this, however, we'll need TensorFlow. The following two blocks are ready to go. The first block creates the model, converts our data into TensorFlow datasets, and defines the loss and optimizer. The second block then trains the model (for two Epochs) while evaluating performance on the training validation sets.\n",
    "\n",
    "For full credit, you need only (a) read through each block, then (b) run them both and observe the result. **Optionally**, you can explore this code further by modifying one or more elements of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import GlobalMaxPool1D, GlobalAveragePooling1D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "\n",
    "# define SWEM\n",
    "class SWEM(Model):\n",
    "    def __init__(self):\n",
    "        super(SWEM, self).__init__()\n",
    "        self.fc1 = Dense(300, activation='relu')\n",
    "        self.mp = GlobalMaxPool1D()\n",
    "        self.ap = GlobalAveragePooling1D()\n",
    "        self.fc2 = Dense(5, activation=None)\n",
    "\n",
    "    def call(self, x):\n",
    "        x = self.fc1(x)\n",
    "        xm = self.mp(x)\n",
    "        xa = self.ap(x)\n",
    "        x = tf.concat([xm, xa], axis=1)\n",
    "        return self.fc2(x)\n",
    "    \n",
    "\n",
    "# create an instance of SWEM\n",
    "model = SWEM()\n",
    "\n",
    "# create tensorflow datasets\n",
    "train_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_train.astype('float32'), y_train)).batch(32)\n",
    "\n",
    "val_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_val.astype('float32'), y_val)).batch(32)\n",
    "\n",
    "test_ds = tf.data.Dataset.from_tensor_slices(\n",
    "    (x_test.astype('float32'), y_test)).batch(32)\n",
    "\n",
    "# multi-class cross-entropy loss\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "\n",
    "# modified stochastic gradient descent optimizer\n",
    "optimizer = tf.keras.optimizers.Adam()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "EPOCHS = 2\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "\n",
    "    train_accuracy = []\n",
    "    val_accuracy = []\n",
    "  \n",
    "    for i, (x, y) in enumerate(train_ds):\n",
    "        \n",
    "        print('Running training batch %i of %i' % (i, len(train_ds)), end='\\r')\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            predicted_logits = model(x)\n",
    "            loss = loss_object(y, predicted_logits)\n",
    "\n",
    "        y_pred = np.argmax(predicted_logits, axis=1)\n",
    "        batch_accuracy = np.mean(y_pred == y)\n",
    "        train_accuracy.append(batch_accuracy)\n",
    "    \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "\n",
    "    for i, (x, y) in enumerate(val_ds):\n",
    "        \n",
    "        print('Running validation batch %i of %i' % (i, len(val_ds)), end='\\r')\n",
    "    \n",
    "        predicted_logits = model(x)\n",
    "        y_pred = np.argmax(predicted_logits, axis=1)\n",
    "    \n",
    "        batch_accuracy = np.mean(y_pred == y)\n",
    "        val_accuracy.append(batch_accuracy)\n",
    "\n",
    "    train_accuracy = 100 * np.mean(train_accuracy)\n",
    "    val_accuracy = 100 * np.mean(val_accuracy)\n",
    "        \n",
    "    print('Epoch %i: train accuracy = %.1f%%, validation accuracy = %.1f%%' % (\n",
    "        epoch, train_accuracy, val_accuracy))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Once you've completed these exercises, please turn in the assignment as follows:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "- download your notebook as html (see File > Download as > HTML (.html))\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS\n",
    "\n",
    "If you're using Google Colab:\n",
    "- download your notebook as .ipynb (see File > Download > Download .ipynb)\n",
    "- if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
