{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GAhB0ibxBJBb"
   },
   "source": [
    "# Computational Exercise 6: Better MNIST Predictions with Tensorflow\n",
    "\n",
    "**Please note that (optionally) this assignment may be completed in groups of 2 students.**\n",
    "\n",
    "**It is recommended that you complete this exercise in Google Colab:**\n",
    "- otherwise you may encounter errors\n",
    "- this will allow you to access free GPU resources\n",
    "\n",
    "---\n",
    "In this exercise, we'll take a look at an important limitation of logistic regression and MLP models applied to images, then overcome these limitations by training a shallow CNN in Tensorflow.\n",
    "\n",
    "Goals are as follows:\n",
    "\n",
    "- Observe that logistic regression is *not* effective when digits are not centered (why?)\n",
    "- Train a simple CNN to identify digits *even when their position varies*\n",
    "- Visualize the filters for each digit learned by the CNN\n",
    "- Create and train a model that classifies handwritten digits with over 98% accuracy\n",
    "\n",
    "We'll begin by importing required libraries:\n",
    "\n",
    "- numpy for efficient math operations\n",
    "- sklearn for defining and training our logistic regression and MLP models\n",
    "- tensorflow for defining and training our simple CNN\n",
    "- matplotlib for visualization/plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "ez69LmdZBJBd"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JxWCcVu_BJBd"
   },
   "source": [
    "As before, we'll load the MNIST dataset. The data are already broken down into:\n",
    "\n",
    "1. a development set, which we'll use for training\n",
    "2. a test set, which we'll use to evaluate performance\n",
    "\n",
    "As in the previous exercise, we will not be tuning our models, so we will not set aside a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B0c4OWUGBJBe",
    "outputId": "b8d3463d-bdb2-46e4-e621-d6f8c1fd98e4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 60000 training images.\n",
      "There are 10000 test images.\n"
     ]
    }
   ],
   "source": [
    "# Load MNIST\n",
    "(x_dev, y_dev), (x_test, y_test) = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "# Normalize values so that they range from 0 to 1\n",
    "x_dev = x_dev / 255.\n",
    "x_test = x_test / 255.\n",
    "\n",
    "print('There are', len(x_dev), 'training images.')\n",
    "print('There are', len(x_test), 'test images.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "by1ydij9BJBe"
   },
   "source": [
    "To review code that allows us to inspect the images and labels, please review [Computational Exercise 5](https://github.com/mengelhard/bsrt_ml4h/blob/master/notebooks/ce5.ipynb)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xqIbwuLQBJBj"
   },
   "source": [
    "## Un-Centering our Digits\n",
    "In MNIST, all digits are centered, which substantially simplifies the problem. In many real-world datasets, on the other hand, we need to identify image features regardless of where they may be present within the image. Let's explore this issue by modifying MNIST so that digits are no longer centered. We'll first enlarge the images, then place digits at random positions within the enlarged image. We'll do this by padding each image (in both our development and test sets) with zeroes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "VXi4BkrBBJBj"
   },
   "outputs": [],
   "source": [
    "def pad_image(img, pad_len):\n",
    "    m, n = img.shape\n",
    "    i = np.random.randint(pad_len - 2) + 1 # pick a horizontal offset between 1 and (pad_len - 1)\n",
    "    j = np.random.randint(pad_len - 2) + 1 # pick a vertical offset bewteen 1 and (pad_len - 1)\n",
    "    img = np.concatenate([[[0] * i] * m, img, [[0] * (pad_len - i)] * m], axis=1) # pad horizontally\n",
    "    img = np.concatenate([[[0] * (n + pad_len)] * j, img, [[0] * (n + pad_len)] * (pad_len - j)], axis=0) # pad vertically\n",
    "    return img\n",
    "\n",
    "x_dev_padded = np.array([pad_image(x, 20) for x in x_dev])\n",
    "x_test_padded = np.array([pad_image(x, 20) for x in x_test])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YAQ6sQywBJBk"
   },
   "source": [
    "In the plots below, we see that digits are no longer centered in the modified dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 241
    },
    "id": "-FbbcU9tBJBk",
    "outputId": "55b17fb9-2d93-4e52-f3ed-471037ec69e9"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x216 with 5 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(nrows=1, ncols=5, figsize=(15, 3))\n",
    "for i in range(5):\n",
    "    ax[i].imshow(x_dev_padded[i, :, :], cmap='gray')\n",
    "    ax[i].set_title('MNIST Image %i, Label = %i' % (i, y_dev[i]), fontsize=12)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uqjCG14RBJBk"
   },
   "source": [
    "## Exercise 1: Logistic Regression\n",
    "Let's see whether logistic regression is able to classify digits effectively now that they're not centered. In the following block, you should:\n",
    "- Train a logistic regression model (i.e. LogisticRegression()) to predict the labels of digits in MNIST.\n",
    "- After training a model on the development set, evaluate its accuracy on the test set. As in the previous computational excercise, you'll need to flatten the images in order to use logistic regression. Remember that your predictions are no longer binary; instead, you are predicting one of ten different digits.\n",
    "- Visualize the logistic regression filters using the same or similar code as in [computational exercise 5](https://github.com/mengelhard/bsrt_ml4h/blob/master/notebooks/ce5.ipynb). Note that since we've padded the images, the filters are now (48, 48) in size rather than (28, 28)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "gVLbmiJLBJBk"
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### CREATE A LOGISTIC REGRESSION MODEL AND TRAIN IT ON THE FLATTENED DEVELOPMENT SET ###\n",
    "\n",
    "\n",
    "### USE THE TRAINED MODEL TO MAKE PREDICTIONS ON THE (FLATTENED) TEST SET ###\n",
    "\n",
    "\n",
    "### EVALUATE THE ACCURACY OF THE MODEL'S PREDICTIONS ON THE TEST SET ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XVuPpQcmBJBl"
   },
   "outputs": [],
   "source": [
    "### ACCESS THE MODEL COEFFICIENTS ###\n",
    "\n",
    "\n",
    "### RESHAPE EACH OF THE 10 COEFFICIENT VECTORS TO HAVE SHAPE (48, 48) ###\n",
    "\n",
    "\n",
    "### PLOT EACH OF THE RESHAPED VECTORS (i.e. FILTERS) ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ld62tsVEBJBl"
   },
   "source": [
    "## Our First Tensorflow Model\n",
    "\n",
    "Let's see if a CNN can do better. In the code below, we'll define and train a very simple CNN that follows an example from our lectures. Specifically, we'll use 10 filters, one for each digit, each of size 28 x 28 -- the same size as the unmodified MNIST images.\n",
    "\n",
    "Note that this CNN is both unusual and inefficient -- normally, CNNs use a larger number of much smaller filters -- but it will allow us to see the *translation invariance* of the CNN.\n",
    "\n",
    "After applying our convolutional layer with unusually large filters, we'll max pool the output of each filter across the whole image, which should tell us whether the digit corresponding to that filter is present *anywhere* within the image. The output of the pooling operations will give us the predicted log-odds for each of the 10 digits.\n",
    "\n",
    "We'll begin by converting our dataset into a tensorflow Dataset object. This accomplishes two things. First, it converts our `numpy` arrays into tensors that we can pass to a Tensorflow model. Second, it will split our test and training datasets into smaller chunks, called batches, that we can iterate over during training and testing. In this case, we'll use batches of size 32, meaning that our Tensorflow model will be processing 32 images at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "wBCbw3mPCl2N"
   },
   "outputs": [],
   "source": [
    "train_ds = tf.data.Dataset.from_tensor_slices((x_dev_padded.astype('float32'), y_dev)).batch(32)\n",
    "test_ds = tf.data.Dataset.from_tensor_slices((x_test_padded.astype('float32'), y_test)).batch(32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "76RDAeWtgEMS"
   },
   "source": [
    "We can then loop over our batches as shown in the next block. Here our goal is just to see what a single batch looks like, so we'll break the loop after printing out the first batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "rkSxcoqICsHN",
    "outputId": "2639bd51-55a1-4668-b624-d5a7c303a6c6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The images have shape (32, 48, 48)\n",
      "The labels have shape (32,)\n"
     ]
    }
   ],
   "source": [
    "for images, labels in train_ds:\n",
    "    print('The images have shape', images.numpy().shape)\n",
    "    print('The labels have shape', labels.numpy().shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VYqZNJU4hJWd"
   },
   "source": [
    "## Defining the Model\n",
    "\n",
    "Now it's time to define our model. We'll begin by importing the different types of layers we'll need, including:\n",
    "1. Dense layers, i.e. fully connected layers. By default, there is no activation function in these layers, but we can specify one (e.g. sigmoid) with the `activation` argument.\n",
    "2. Conv2D layers, i.e. convolutional layers. The first two arguments when defining such a layer are the number and size of the filters, respectively.\n",
    "3. MaxPool2D layers, i.e. max pooling layers. The first argument specifies the size (i.e. width and length) of the pooling window.\n",
    "4. Flatten layers simply flatten the tensor similar to `.flatten` in `numpy`.\n",
    "\n",
    "We can then import the Tensorflow `Model` class, which provides a foundation for the model we'll be defining.\n",
    "\n",
    "In the code below, we'll define our new, `SimpleCNN` class based on the model class. Really understanding what's happening here requires you to understand a little bit about classes in Python. Ultimately, though, there are a few important things to note.\n",
    "\n",
    "- You'll follow this general template every time you build your own Tensorflow model.\n",
    "- Each of the layers in your model will be defined in the `__init__` method. Here we'll define a single Conv2D layer called `conv1`, a single MaxPool2D layer called `mp1`, and a single Flatten layer called `flatten`. Prefacing these new layers with `self.` (e.g. `self.conv1`) specifies that they are attributes of the model; in other words, they are *attached* to the model much like `.coef_` was an attribute of LogisticRegression.\n",
    "- The `call` method then defines how the layers are successively applied to transform input into hidden (i.e. latent variables) and eventually into predictions. Here we first add a new axis to the input; this is needed because `Conv2D` expects a `channels` axis (e.g. RGB color channels). We then apply our convolutional layer, followed by our max-pool layer, and finally we flatten everything, resulting in a vector of 10 different log-odds values corresponding to our 10 digits."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "dXS8_qFSBJBl"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPool2D\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "class SimpleCNN(Model):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = Conv2D(10, 28, use_bias=False)\n",
    "        self.mp1 = MaxPool2D(21)\n",
    "        self.flatten = Flatten()\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x[..., tf.newaxis]\n",
    "        x = self.conv1(x)\n",
    "        x = self.mp1(x)\n",
    "        return self.flatten(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NJYOs_fQlv7K"
   },
   "source": [
    "We can then create an instance of our `SimpleCNN`, just as we did when using `LogisticRegression` or `MLPClassifier` in `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "1JT6n8ZjTfSD"
   },
   "outputs": [],
   "source": [
    "model = SimpleCNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5f6jkLM2l_Ee"
   },
   "source": [
    "There's one more step, and then we can start training the model.\n",
    "- First, we'll define a `loss_object` that will make it easy for us to calculate the cross-entropy loss based on (a) the model-predicted logits, and (b) the true labels in our training set.\n",
    "- Then, we'll need to define an optimizer that will determine exactly how our model's parameters should be altered based on the gradient of the loss, which Tensorflow will calculate automatically using `loss_object`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "QotAGwTRTd3S"
   },
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True) # multi-class cross-entropy loss\n",
    "optimizer = tf.keras.optimizers.Adam() # modified stochastic gradient descent optimizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5PRlxAvBJBm"
   },
   "source": [
    "We're now ready to train. We'll loop over all the batches in our training set 3 times in a row (i.e. 3 EPOCHS). For each batch, we'll:\n",
    "1. apply our model to generate predictions for that batch\n",
    "2. calculate the loss based on those predictions\n",
    "3. calculate the gradient of the loss (i.e. how much does loss change in respose to changes in each parameter)\n",
    "4. use our optimizer to adjust the model parameters (i.e. `model.trainable_variables`) based on the gradient.\n",
    "- Note that `tf.GradientTape()` is just a quirk of Tensorflow that tells Tensorflow to watch the model parameters as it calculates the loss.\n",
    "\n",
    "**Important**: This code will be very slow unless you switch to a GPU runtime from the Colab menu. Even with this change, it'll still take a minute or two.\n",
    "- Runtime > Change runtime type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "DvYkMdHwBJBn",
    "outputId": "cb5de380-254c-44da-de52-15e219ab7fa6",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completed epoch 0\n",
      "Completed epoch 1\n",
      "Completed epoch 2\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  \n",
    "    for images, labels in train_ds:\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images)\n",
    "            loss = loss_object(labels, predictions)\n",
    "    \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    print('Completed epoch', epoch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rP-mHKx7pFjT"
   },
   "source": [
    "Having trained our model, we can now evaluate its accuracy on the training set. This is very similar to code we've used before, but here we're calculating accuracy batch by batch, then averaging the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "D9s2RUaYa-Kd",
    "outputId": "e95163c9-0898-434b-b310-7ce5e90b51c4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.931\n"
     ]
    }
   ],
   "source": [
    "accuracy = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    \n",
    "    predicted_logits = model(images)\n",
    "    predicted_labels = np.argmax(predicted_logits, axis=1)\n",
    "    \n",
    "    batch_accuracy = np.mean(predicted_labels == labels)\n",
    "    accuracy.append(batch_accuracy)\n",
    "\n",
    "accuracy = np.mean(accuracy)\n",
    "print('The accuracy is %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J2S9_qsVpmkh"
   },
   "source": [
    "## Exercise 2: Visualize the filters learned by the CNN\n",
    "The filters of the trained model can be accessed as an attribute of the `model.conv1` layer, as shown in the first line of the block below. `filters` will be an array of size (10, 28, 28) corresponding to the 10 filters -- one for each digit -- each of which is the same size (28 by 28) as the images. In the following block, you should plot and inspect these filters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "id": "mj-cTTKEbhzd"
   },
   "outputs": [],
   "source": [
    "filters = np.moveaxis(np.squeeze(model.conv1.get_weights()[0]), [0, 1, 2], [1, 2, 0])\n",
    "\n",
    "### PLOT EACH OF THE 10 28 by 28 FILTERS ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dOiUVhscBJBp"
   },
   "source": [
    "## Exercise 3: A better CNN.\n",
    "\n",
    "**Note: This exercise is optional for those who were not already familiar with Python at the beginning of the course**\n",
    "\n",
    "The CNN we defined was interesting, but it's not very efficient. In short, 28 by 28 filters are too big, and although this architecture is good for illustrating translation invariance, its accuracy can be improved.\n",
    "\n",
    "In this exercise, you'll build a better CNN, train it, and evaluate it.\n",
    "- Create the model by following the prompts in the code block below.\n",
    "- Then, duplicate code blocks above to create an instance of `BetterCNN`, train it, and evaluate its performance on the test set.\n",
    "\n",
    "The following may also be interesting, though none is required:\n",
    "- visualize the learned filters in the convolutional layer\n",
    "- modify the architecture to see if performance can be further improved"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AGcHdYWeuOyI"
   },
   "outputs": [],
   "source": [
    "class BetterCNN(Model):\n",
    "    def __init__(self):\n",
    "        super(BetterCNN, self).__init__()\n",
    "        ### DEFINE A CONVOLUTIONAL LAYER WITH 32 FILTERS OF SIZE 3 (i.e. 3x3) AND RELU ACTIVATION (i.e. activation='relu') ###\n",
    "        ### DEFINE A MAX POOL LAYER WITH 2x2 POOL SIZE ###\n",
    "        ### DEFINE A SECOND CONVOLUTIONAL LAYER WITH 32 FILTERS OF SIZE 5 AND RELU ACTIVATION ###\n",
    "        ### DEFINE A FLATTEN LAYER ###\n",
    "        ### DEFINE A DENSE LAYER WITH OUTPUT SIZE 128 AND RELU ACTIVATION (i.e. activation='relu') ###\n",
    "        ### DEFINE A DENSE LAYER WITH OUTPUT SIZE 10 AND NO ACTIVATION. THIS IS THE FINAL LAYER; OUTPUT WILL BE THE LOG-ODDS ###\n",
    "\n",
    "    def call(self, x):\n",
    "        x = x[..., tf.newaxis]\n",
    "        #x = ### APPLY FIRST CONVOLUTIONAL LAYER ###\n",
    "        #x = ### APPLY MAX POOL LAYER ###\n",
    "        #x = ### APPLY SECOND CONVOLUTIONAL LAYER ###\n",
    "        #x = ### APPLY FLATTEN LAYER ###\n",
    "        #x = ### APPLY FIRST DENSE LAYER ###\n",
    "        #x = ### APPLY SECOND DENSE LAYER ###\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Swp_ZOQ3BJBr"
   },
   "source": [
    "The following blocks, repeated from above, can now be used to train and evaluate `BetterCNN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gxeo7qKiykwr"
   },
   "outputs": [],
   "source": [
    "model = BetterCNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dH92qMY6xfpu"
   },
   "outputs": [],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "  \n",
    "    for images, labels in train_ds:\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = model(images)\n",
    "            loss = loss_object(labels, predictions)\n",
    "    \n",
    "        gradients = tape.gradient(loss, model.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "        \n",
    "    print('Completed epoch', epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4bhlkdalxjC7"
   },
   "outputs": [],
   "source": [
    "accuracy = []\n",
    "\n",
    "for images, labels in test_ds:\n",
    "    \n",
    "    predicted_logits = model(images)\n",
    "    predicted_labels = np.argmax(predicted_logits, axis=1)\n",
    "    \n",
    "    batch_accuracy = np.mean(predicted_labels == labels)\n",
    "    accuracy.append(batch_accuracy)\n",
    "\n",
    "accuracy = np.mean(accuracy)\n",
    "print('The accuracy is %.3f' % accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iesZQUopBJBs"
   },
   "source": [
    "### Once you've completed these exercises, please turn in the assignment as follows:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "- download your notebook as html (see File > Download as > HTML (.html))\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS\n",
    "\n",
    "If you're using Google Colab:\n",
    "- download your notebook as .ipynb (see File > Download > Download .ipynb)\n",
    "- if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
    "- .zip the file (i.e. place it in a .zip archive)\n",
    "- submit the .zip file in Talent LMS"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ce6.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "tf2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
