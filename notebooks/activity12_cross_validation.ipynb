{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "pressed-massage",
   "metadata": {},
   "source": [
    "# Activity 12: Cross Validation\n",
    "\n",
    "**Please note that (optionally) this assignment may be completed in groups of 2 students.**\n",
    "\n",
    "---\n",
    "In this assignment, we'll develop models that predict malignancy in the [Wisconsin Breast Cancer Diagnosis Dataset][1] we explored last time.\n",
    "\n",
    "As before, please note that there are a number of resources related to this dataset, including the following:\n",
    "- [discussion and examples on kaggle][2]\n",
    "- [Medium article similar to this assignment][3]\n",
    "\n",
    "Goals are as follows:\n",
    "\n",
    "- Continue to gain familiarity with Python and the Jupyter notebook format\n",
    "- See how to prepare a simple, modeling-friendly dataset for model development\n",
    "- See how to train and evaluate logistic regression and multilayer perceptron models can be trained and evaluated in `sklearn`\n",
    "- Begin to interpret and contextualize model performance\n",
    "\n",
    "Each of our computational assignments will begin by importing a few required libraries using an `import` statement. These libraries extend the basic functionality of Python. By importing `as X` (e.g. `as np`), we can shorten subsequent calls to the library in our code.\n",
    "\n",
    "- `numpy` for efficient math operations\n",
    "- `pandas` for dataframes and dataframe operations\n",
    "- `matplotlib` for visualization/plotting\n",
    "- `sklearn` gives us a convenient way to load our dataset, as before, but this time we'll also use it to develop our models! The vast majority of \"standard\" machine learning models are implemented in this library. When working with specific neural network architectures, on the other hand -- this includes convolutional and recurrent neural networks -- we'll need to use a more customizable machine learning library like `tensorflow` or `pytorch`.\n",
    "\n",
    "[1]: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Wisconsin+(Diagnostic)\n",
    "[2]: https://www.kaggle.com/shubamsumbria/breast-cancer-prediction\n",
    "[3]: https://medium.com/analytics-vidhya/breast-cancer-diagnostic-dataset-eda-fa0de80f15bd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "conscious-ecology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-postage",
   "metadata": {},
   "source": [
    "## Loading the data\n",
    "\n",
    "As in assignment 1, we'll use `sklearn` to load the dataset. Typically you might load from `.csv` with `pd.read_csv()`, from `.xlsx` with `pd.read_excel()`, etc., but the result would be the same: you'd end up with a `pandas` dataframe. In this case, `sklearn` gives us a nice way to load this dataframe without having to find and download a `.csv` file on our own.\n",
    "\n",
    "**Important**: if you encounter an error with `load_breast_cancer`, try upgrading `sklearn` by adding a code block with the following:\n",
    "> `!pip install --upgrade scikit-learn`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "missing-chemical",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_breast_cancer\n",
    "df, y_true = load_breast_cancer(return_X_y=True, as_frame=True)\n",
    "y_true = 1 - y_true # let's set benign to 0 and malignant to 1, in keeping with usual conventions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57dac177",
   "metadata": {},
   "source": [
    "It turns out predicting `y` from these data is a bit too easy. To illustrate a few important concepts, such as overfitting, we need to make the problem more difficult by adding some randomness to the labels.\n",
    "\n",
    "We can do this by creating a function, `flip_some_labels`, that will flip a portion of the labels `y_true` at random, resulting in the (noisier) labels `y`. In a subsequent assignment, we'll return to the true labels `y_true` to see how well we can actually predict malignancy from these data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1faa95f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def flip_some_labels(labels, flip_rate=.1, random_seed=0):\n",
    "    return (labels + (np.random.RandomState(random_seed).rand(len(labels)) < flip_rate)) % 2\n",
    "\n",
    "y = flip_some_labels(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polished-stage",
   "metadata": {},
   "source": [
    "We now have two objects: a dataframe `df` of predictors, and a single *series* (i.e. column) `y` of the associated outcomes. Since we explored this dataset last time, we can skip the descriptive statistics and plots. Still, let's do a quick `.head()` check just to make sure nothing has changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "simple-belly",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mean radius</th>\n",
       "      <th>mean texture</th>\n",
       "      <th>mean perimeter</th>\n",
       "      <th>mean area</th>\n",
       "      <th>mean smoothness</th>\n",
       "      <th>mean compactness</th>\n",
       "      <th>mean concavity</th>\n",
       "      <th>mean concave points</th>\n",
       "      <th>mean symmetry</th>\n",
       "      <th>mean fractal dimension</th>\n",
       "      <th>...</th>\n",
       "      <th>worst radius</th>\n",
       "      <th>worst texture</th>\n",
       "      <th>worst perimeter</th>\n",
       "      <th>worst area</th>\n",
       "      <th>worst smoothness</th>\n",
       "      <th>worst compactness</th>\n",
       "      <th>worst concavity</th>\n",
       "      <th>worst concave points</th>\n",
       "      <th>worst symmetry</th>\n",
       "      <th>worst fractal dimension</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>0.07871</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>0.05667</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>0.05999</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>0.09744</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>0.05883</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
       "0        17.99         10.38          122.80     1001.0          0.11840   \n",
       "1        20.57         17.77          132.90     1326.0          0.08474   \n",
       "2        19.69         21.25          130.00     1203.0          0.10960   \n",
       "3        11.42         20.38           77.58      386.1          0.14250   \n",
       "4        20.29         14.34          135.10     1297.0          0.10030   \n",
       "\n",
       "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
       "0           0.27760          0.3001              0.14710         0.2419   \n",
       "1           0.07864          0.0869              0.07017         0.1812   \n",
       "2           0.15990          0.1974              0.12790         0.2069   \n",
       "3           0.28390          0.2414              0.10520         0.2597   \n",
       "4           0.13280          0.1980              0.10430         0.1809   \n",
       "\n",
       "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
       "0                 0.07871  ...         25.38          17.33           184.60   \n",
       "1                 0.05667  ...         24.99          23.41           158.80   \n",
       "2                 0.05999  ...         23.57          25.53           152.50   \n",
       "3                 0.09744  ...         14.91          26.50            98.87   \n",
       "4                 0.05883  ...         22.54          16.67           152.20   \n",
       "\n",
       "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
       "0      2019.0            0.1622             0.6656           0.7119   \n",
       "1      1956.0            0.1238             0.1866           0.2416   \n",
       "2      1709.0            0.1444             0.4245           0.4504   \n",
       "3       567.7            0.2098             0.8663           0.6869   \n",
       "4      1575.0            0.1374             0.2050           0.4000   \n",
       "\n",
       "   worst concave points  worst symmetry  worst fractal dimension  \n",
       "0                0.2654          0.4601                  0.11890  \n",
       "1                0.1860          0.2750                  0.08902  \n",
       "2                0.2430          0.3613                  0.08758  \n",
       "3                0.2575          0.6638                  0.17300  \n",
       "4                0.1625          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58ee675a",
   "metadata": {},
   "source": [
    "We'll also check our outcomes with `.value_counts()`. Compared to the previous exercise, about 10% of the labels have been flipped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "underlying-duration",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    340\n",
       "1    229\n",
       "Name: target, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latin-grade",
   "metadata": {},
   "source": [
    "Since we explored this dataset in our previous assignment, we can now take a few more simple steps to prepare it for model development. First, note that there are 569 patients in our dataset. We can determine this by adding the value counts for y, or with `len(df)`. To keep things simple, let's **use the first 400 samples (i.e. rows) to *train* our models, and the remaining 169 to *test* it**. We will *not* be defining a validation set in this exercise. Later on, we'll be using a validation set to select a best model or tune hyperparameters, but we don't need to worry about this just yet.\n",
    "\n",
    "The final step prior to modeling will be to *standardize* our data by shifting it so that the mean is 0, then scaling it so that the standard deviation is 1. This step will make our model coefficients more interpretable and keep them all in the same range; the latter is particularly important for neural networks, and for models in which large coefficients are penalized.\n",
    "\n",
    "All the features in this dataset are numeric, so (a) we won't have to worry about preparing categorical features for modeling, and (b) all of the features can (and should) be standardized.\n",
    "\n",
    "We need to standardize both our training set and our test set. However, **the test set should be standardized using the mean and standard deviation from the *training* set**. It is useful to think of standardization as part of our model, and we don't want to use **any** information from the test set -- not even its mean or standard deviation -- in the model development process.\n",
    "\n",
    "## Exercise 1: Partition and Standardize\n",
    "\n",
    "In the following block, you should:\n",
    "1. divide the data and labels into a training set and test set. Note that `df[:N]` selects the first N rows, and `df[N:]` selects the remaining rows.\n",
    "2. standardize both sets of data using the mean and standard deviation *from the training set* using the same technique you used in the exercises from last week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cognitive-parker",
   "metadata": {},
   "outputs": [],
   "source": [
    "### DIVIDE THE DATA INTO FOLDS ###\n",
    "\n",
    "def create_fold_index(N_samples, N_folds, random_state=0):\n",
    "    return np.random.RandomState(random_state).permutation(N_samples) * N_folds // N_samples\n",
    "\n",
    "fold_idx = create_fold_index(len(df), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6219fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(5):\n",
    "    \n",
    "    x_train = df[fold_idx != i]\n",
    "    x_test = df[fold_idx == i]\n",
    "    \n",
    "    y_train = y[fold_idx != i]\n",
    "    y_test = y[fold_idx == i]\n",
    "    \n",
    "    ### STANDARDIZE THE DATA ###\n",
    "    \n",
    "    \n",
    "    ### TRAIN THE MODEL ON THE TRAINING SET ###\n",
    "    \n",
    "    \n",
    "    ### EVALUATE PERFORMANCE ON THE TEST SET ###\n",
    "    \n",
    "    \n",
    "    ### SAVE THE PERFORMANCE FIGURES ###"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinguished-methodology",
   "metadata": {},
   "source": [
    "## Training a First Model\n",
    "\n",
    "We're finally ready to train and evaluate our first model: logistic regression. There are only a few lines of code in the block below, but each one is important.\n",
    "- In the first line, we create a `LogisticRegression()` model object. This is our model; we can train it, then use it to make predictions. All information about the model and its parameters is stored within the object. See [the documentation](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) for further details.\n",
    "- We'll pass a `random_state` parameter when initializing *all* of our models to ensure that we'll get a consistent result in cases where there would otherwise be randomness in training and/or the initialization of parameters. We'll also specify that model parameters should not be penalized in any way by passing `penalty='none'`, and ensure the model has suffient time to finish training by passing `max_iter=10000`.\n",
    "- In the second line, we will use the `.fit()` method on our model object to fit the model to our training set. In other words, this very short line does all the work of actually training the model. For more complex models, this line may take some time to run.\n",
    "- In the third line, we predict the *probability* that `y` is 1 for each of the samples in our test set. By default, `sklearn` returns two columns corresponding to the predicted probability that y is 0 and 1, respectively. We only need the latter, so we'll select this column with `[:, 1]`.\n",
    "- In the fourth line, we predict the *value* of `y` based on this probability. Specifically, we'll predict that `y` is 1 whenever the predicted probability is greater than 0.5, otherwise we'll predict that `y` is 0. Using 0.5 as our threshold is not always the best idea, but it'll work for now.\n",
    "\n",
    "We'll repeat these steps, with minor variations, each time we train a model.\n",
    "\n",
    "## Exercise 2: Fit and Predict\n",
    "Modify the following block to train your model on the training set you created, then make predictions on the test set. The changes you need to make here are minor, but understanding what is happening in these lines is important, and this syntax will be repeated in subsequent exercises. Please consult the explanations above as you work through this block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "major-bracket",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "### FIRST LINE: No changes needed\n",
    "lr_model = LogisticRegression(random_state=0, penalty='none', max_iter=10000)\n",
    "\n",
    "### SECOND LINE: change X_train and y_train to the variable names you've created, then uncomment and run\n",
    "# lr_model.fit(X_train, y_train)\n",
    "\n",
    "### THIRD LINE: change X_test to the variable name you used, then uncomment and run\n",
    "# y_test_pred_proba = lr_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "### FOURTH LINE: No changes needed; simply uncomment\n",
    "# y_test_pred_label = (y_test_pred_proba > .5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73516e85",
   "metadata": {},
   "source": [
    "## Evaluating Performance\n",
    "\n",
    "We can now evaluate our model by comparing its predictions to the true labels in the test set. Here we'll focus on two measures of performance:\n",
    "- Accuracy, which has some important limitations but is convenient and easy to calculate\n",
    "- Area under the receiver operating characteristic curve (AUC or AUROC)\n",
    "\n",
    "We'll be learning more about the AUC in upcoming lectures and exercises. For now, what's important it that:\n",
    "1. This is a common and useful performance metric.\n",
    "2. It's based on comparing model-predicted probabilities (`y_test_pred_proba`) to the labels (`y`).\n",
    "3. We can calculate it using `roc_auc_score` function from `sklearn.metrics`.\n",
    "\n",
    "## Exercise 3: Accuracy and AUC\n",
    "In the following block, you should:\n",
    "- calculate the accuracy by comparing predicted labels (`y_test_pred_labels`) to `y`, counting how many times they match, and dividing by the total length (i.e. number of labels)\n",
    "- calculate the AUC by applying the `roc_auc_score` function to `y` and `y_test_pred_proba`. The documentation for `roc_auc_score` is [here](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_auc_score.html) if needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "445f4dc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "### CALCULATE THE ACCURACY ON THE TEST SET AND PRINT THE RESULT ###\n",
    "\n",
    "\n",
    "### CALCULATE THE AUC ON THE TEST SET AND PRINT THE RESULT ###\n",
    "# roc_auc_score(y, y_test_pred_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a580e16e",
   "metadata": {},
   "source": [
    "## Exercise 4: MLP\n",
    "\n",
    "In the following block, you will train a new model: a multilayer perceptron with a single, wide hidden layer.\n",
    "- The model itself has been defined in the first two lines. We're using the `hidden_layer_sizes` parameter to tell `MLPClassifier` that we want an MLP with a single hidden layer of size 1000. If we wanted two hidden layers each of size 100, we'd pass `hidden_layer_sizes=(100, 100)`.\n",
    "- As before, we use the `random_state` parameter to ensure we get a consistent result even if we run the block twice, for instance. Passing `max_iter=10000` ensures the model has enough time to finish training.\n",
    "- Adapt the code from exercise 2 (above) to train the new model, then make predictions (both probability and label) on the *training* set.\n",
    "- Adapt your code from exercise 3 (above) to evaluate the accuracy and AUC of these predictions\n",
    "- In one sentence, state whether the MLP performed better than logistic regression, then list at least one characteristic of the model or data that may partly explain why this is the case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc1756c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neural_network import MLPClassifier\n",
    "\n",
    "mlp_model = MLPClassifier(hidden_layer_sizes=(1000,), random_state=0, max_iter=10000)\n",
    "\n",
    "\n",
    "### TRAIN THE MODEL ON THE TRAINING SET, THEN MAKE PREDICTIONS ON THE TEST SET ###\n",
    "\n",
    "\n",
    "\n",
    "### CALCULATE ACCURACY AND AUC ON THE TEST SET, THEN PRINT THE RESULT ###\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a23f237b",
   "metadata": {},
   "source": [
    "## Once you've completed these exercises, please turn in the assignment as follows:\n",
    "\n",
    "If you're using Anaconda on your local machine:\n",
    "1. download your notebook as html (see `File > Download as > HTML (.html)`)\n",
    "2. .zip the file (i.e. place it in a .zip archive)\n",
    "3. submit the .zip file in Talent LMS\n",
    "\n",
    "If you're using Google Colab:\n",
    "1. download your notebook as .ipynb (see `File > Download > Download .ipynb`)\n",
    "2. if you have nbconvert installed, convert it to .html; if not, leave is as .ipynb\n",
    "3. .zip the file (i.e. place it in a .zip archive)\n",
    "4. submit the .zip file in Talent LMS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df208bdb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
